{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40f4187e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Topic Modeling for Yelp Business Success\n",
    "# Author: Farzam Afzal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3b6ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# NLP libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Topic Modeling\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.sklearn\n",
    "\n",
    "# BERT Sentiment Analysis\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "734b08bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    }
   ],
   "source": [
    "# Downloading necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Setting paths for data files\n",
    "review_path = 'yelp_academic_dataset_review.json'  # Update with actual path\n",
    "business_path = 'yelp_academic_dataset_business.json'  # Update with actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01847269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading JSON files line by line\n",
    "def read_json(file_path, max_records=None):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(tqdm(f)):\n",
    "            if max_records and i >= max_records:\n",
    "                break\n",
    "            data.append(json.loads(line))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Loading a sample of review data for development (adjust max_records for full implementation)\n",
    "print(\"Loading review data...\")\n",
    "review_df = read_json(review_path, max_records=50000)  # May use a larger sample for actual implementation\n",
    "print(f\"Loaded {len(review_df)} reviews.\")\n",
    "\n",
    "# Load business data to join with reviews\n",
    "print(\"Loading business data...\")\n",
    "business_df = read_json(business_path)\n",
    "print(f\"Loaded {len(business_df)} businesses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df94d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining business categories with reviews for context-aware analysis\n",
    "review_business_df = review_df.merge(\n",
    "    business_df[['business_id', 'categories']], \n",
    "    on='business_id', \n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628f67fb",
   "metadata": {},
   "source": [
    "#--------------------------------------------------\n",
    "# Text Preprocessing Functions\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and normalize text data for NLP tasks.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Conversion to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removal of HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # Removal of URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Removal of special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Removal of extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_and_lemmatize(text):\n",
    "    \"\"\"Tokenize and lemmatize text.\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Removing stopwords and lemmatizing\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630a256c",
   "metadata": {},
   "source": [
    "#--------------------------------------------------\n",
    "# BERT-based Sentiment Analysis\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f9a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Load pre-trained BERT model for sentiment analysis\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "        self.sentiment_pipeline = pipeline(\"sentiment-analysis\", \n",
    "                                           model=self.model, \n",
    "                                           tokenizer=self.tokenizer)\n",
    "        \n",
    "    def analyze_sentiment(self, text, max_length=512):\n",
    "        \"\"\"\n",
    "        Analyze sentiment of text using BERT.\n",
    "        Returns:\n",
    "            - sentiment_score: Normalized score between 0 and 1\n",
    "            - sentiment_label: One of 'negative', 'neutral', or 'positive'\n",
    "        \"\"\"\n",
    "        # Truncates text to max_length\n",
    "        text = text[:max_length]\n",
    "        \n",
    "        # Gets sentiment prediction\n",
    "        result = self.sentiment_pipeline(text)[0]\n",
    "        \n",
    "        # Extracts label and score\n",
    "        raw_score = int(result['label'].split(' ')[0])\n",
    "        \n",
    "        # Normalizes to 0-1 range (model returns scores 1-5)\n",
    "        normalized_score = (raw_score - 1) / 4\n",
    "        \n",
    "        # Determines sentiment label\n",
    "        if normalized_score < 0.4:\n",
    "            sentiment_label = 'negative'\n",
    "        elif normalized_score > 0.6:\n",
    "            sentiment_label = 'positive'\n",
    "        else:\n",
    "            sentiment_label = 'neutral'\n",
    "            \n",
    "        return normalized_score, sentiment_label\n",
    "    \n",
    "    def analyze_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Analyze sentiment for a batch of texts.\"\"\"\n",
    "        sentiment_scores = []\n",
    "        sentiment_labels = []\n",
    "        \n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            results = []\n",
    "            \n",
    "            for text in tqdm(batch, desc=f\"Processing batch {i//batch_size + 1}\"):\n",
    "                if isinstance(text, str) and len(text.strip()) > 0:\n",
    "                    score, label = self.analyze_sentiment(text)\n",
    "                else:\n",
    "                    score, label = 0.5, 'neutral'  # Default for empty text\n",
    "                \n",
    "                results.append((score, label))\n",
    "            \n",
    "            batch_scores, batch_labels = zip(*results)\n",
    "            sentiment_scores.extend(batch_scores)\n",
    "            sentiment_labels.extend(batch_labels)\n",
    "        \n",
    "        return sentiment_scores, sentiment_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16354c85",
   "metadata": {},
   "source": [
    "#--------------------------------------------------\n",
    "# Topic Modeling with LDA\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewTopicModeler:\n",
    "    def __init__(self, n_topics=10, max_features=5000, min_df=5, max_df=0.8):\n",
    "        self.n_topics = n_topics\n",
    "        self.max_features = max_features\n",
    "        self.min_df = min_df\n",
    "        self.max_df = max_df\n",
    "        \n",
    "        # Initializes vectorizer\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            max_features=max_features,\n",
    "            min_df=min_df,\n",
    "            max_df=max_df,\n",
    "            stop_words='english'\n",
    "        )\n",
    "        \n",
    "        # Initializes LDA model\n",
    "        self.lda_model = LatentDirichletAllocation(\n",
    "            n_components=n_topics,\n",
    "            random_state=42,\n",
    "            max_iter=10,\n",
    "            learning_method='online'\n",
    "        )\n",
    "        \n",
    "        self.feature_names = None\n",
    "        self.document_topics = None\n",
    "        \n",
    "    def fit_transform(self, texts):\n",
    "        \"\"\"Fit LDA model and transform texts to topic distributions.\"\"\"\n",
    "        # Creates document-term matrix\n",
    "        print(\"Creating document-term matrix...\")\n",
    "        dtm = self.vectorizer.fit_transform(texts)\n",
    "        self.feature_names = self.vectorizer.get_feature_names_out()\n",
    "        \n",
    "        # Fits LDA model\n",
    "        print(f\"Fitting LDA model with {self.n_topics} topics...\")\n",
    "        self.document_topics = self.lda_model.fit_transform(dtm)\n",
    "        \n",
    "        return self.document_topics\n",
    "    \n",
    "    def transform(self, texts):\n",
    "        \"\"\"Transform new texts to topic distributions.\"\"\"\n",
    "        dtm = self.vectorizer.transform(texts)\n",
    "        return self.lda_model.transform(dtm)\n",
    "    \n",
    "    def get_topic_words(self, n_words=10):\n",
    "        \"\"\"Get the top words for each topic.\"\"\"\n",
    "        topic_words = []\n",
    "        \n",
    "        for topic_idx, topic in enumerate(self.lda_model.components_):\n",
    "            top_word_indices = topic.argsort()[:-n_words-1:-1]\n",
    "            top_words = [self.feature_names[i] for i in top_word_indices]\n",
    "            topic_words.append(top_words)\n",
    "            \n",
    "        return topic_words\n",
    "    \n",
    "    def visualize_topics(self, dtm=None):\n",
    "        \"\"\"Create an interactive visualization of topics.\"\"\"\n",
    "        if dtm is None:\n",
    "            # Uses the document-term matrix from fit_transform\n",
    "            dtm = self.vectorizer.transform(texts)\n",
    "            \n",
    "        # Prepares visualization\n",
    "        vis_data = pyLDAvis.sklearn.prepare(\n",
    "            self.lda_model, \n",
    "            dtm, \n",
    "            self.vectorizer,\n",
    "            mds='tsne'\n",
    "        )\n",
    "        \n",
    "        # Saves visualization to HTML\n",
    "        pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n",
    "        print(\"Visualization saved to 'lda_visualization.html'\")\n",
    "        \n",
    "        return vis_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf34bc1",
   "metadata": {},
   "source": [
    "#--------------------------------------------------\n",
    "# Main Analysis Pipeline\n",
    "#--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8b16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Sampling a subset of reviews for development (remove this limit for full implementation)\n",
    "    sample_size = 10000\n",
    "    print(f\"Sampling {sample_size} reviews for analysis...\")\n",
    "    review_sample = review_business_df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Preprocessing review text\n",
    "    print(\"Preprocessing review text...\")\n",
    "    review_sample['processed_text'] = review_sample['text'].apply(preprocess_text)\n",
    "    \n",
    "    # Filters out empty reviews after preprocessing\n",
    "    review_sample = review_sample[review_sample['processed_text'].str.len() > 20]\n",
    "    print(f\"After preprocessing, {len(review_sample)} reviews remain.\")\n",
    "    \n",
    "    # Initializes BERT sentiment analyzer\n",
    "    print(\"Initializing BERT sentiment analyzer...\")\n",
    "    sentiment_analyzer = BERTSentimentAnalyzer()\n",
    "    \n",
    "    # Analyzes sentiment for reviews\n",
    "    print(\"Analyzing sentiment...\")\n",
    "    sentiment_scores, sentiment_labels = sentiment_analyzer.analyze_batch(\n",
    "        review_sample['processed_text'].tolist()\n",
    "    )\n",
    "    \n",
    "    # Adds sentiment analysis results to dataframe\n",
    "    review_sample['sentiment_score'] = sentiment_scores\n",
    "    review_sample['sentiment_label'] = sentiment_labels\n",
    "    \n",
    "    # Initializes topic modeler\n",
    "    print(\"Initializing topic modeler...\")\n",
    "    topic_modeler = ReviewTopicModeler(n_topics=10)\n",
    "    \n",
    "    # Fits topic model and get topic distributions\n",
    "    topic_distributions = topic_modeler.fit_transform(review_sample['processed_text'])\n",
    "    \n",
    "    # Adds dominant topic for each review\n",
    "    dominant_topics = np.argmax(topic_distributions, axis=1)\n",
    "    review_sample['dominant_topic'] = dominant_topics\n",
    "    \n",
    "    # Gets top words for each topic\n",
    "    topic_words = topic_modeler.get_topic_words(n_words=15)\n",
    "    \n",
    "    # Prints topic keywords\n",
    "    print(\"\\nTopic Keywords:\")\n",
    "    for i, words in enumerate(topic_words):\n",
    "        print(f\"Topic {i}: {', '.join(words)}\")\n",
    "        \n",
    "    # Analyzes relationship between sentiment and topics\n",
    "    topic_sentiment = review_sample.groupby('dominant_topic')['sentiment_score'].mean()\n",
    "    \n",
    "    print(\"\\nAverage Sentiment Score by Topic:\")\n",
    "    print(topic_sentiment)\n",
    "    \n",
    "    # Analyzes relationship between topics and business stars\n",
    "    topic_stars = review_sample.groupby('dominant_topic')['stars'].mean()\n",
    "    \n",
    "    print(\"\\nAverage Star Rating by Topic:\")\n",
    "    print(topic_stars)\n",
    "    \n",
    "    # Saves processed data for further analysis\n",
    "    print(\"Saving processed data...\")\n",
    "    review_sample.to_csv('processed_reviews_with_nlp.csv', index=False)\n",
    "    \n",
    "    # Creates sentiment score distribution by business\n",
    "    business_sentiment = review_sample.groupby('business_id')['sentiment_score'].agg(['mean', 'median', 'count'])\n",
    "    business_sentiment = business_sentiment.rename(columns={'mean': 'avg_sentiment', 'count': 'review_count'})\n",
    "    \n",
    "    # Merges with business data\n",
    "    business_nlp_df = business_df.merge(business_sentiment, on='business_id', how='inner')\n",
    "    \n",
    "    # Saves business-level sentiment data\n",
    "    business_nlp_df.to_csv('business_sentiment_data.csv', index=False)\n",
    "    \n",
    "    print(\"Analysis complete. Results saved to CSV files.\")\n",
    "    \n",
    "    # Creates visualizations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(review_sample['sentiment_score'], bins=20, kde=True)\n",
    "    plt.title('Distribution of Sentiment Scores')\n",
    "    plt.xlabel('Sentiment Score (0-1)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig('sentiment_distribution.png')\n",
    "    \n",
    "    # Topic-Sentiment relationship visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    topic_sentiment.plot(kind='bar')\n",
    "    plt.title('Average Sentiment Score by Topic')\n",
    "    plt.xlabel('Topic ID')\n",
    "    plt.ylabel('Average Sentiment Score')\n",
    "    plt.savefig('topic_sentiment.png')\n",
    "    \n",
    "    # Generating interactive LDA visualization\n",
    "    topic_modeler.visualize_topics()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
